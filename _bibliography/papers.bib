---
---


@article{sen2023hyp,
  title={HyP-NeRF: Learning Improved NeRF Priors using a HyperNetwork},
  author={{Bipsaha Sen*,} {Gaurav Singh*,} {Aditya Agarwal*,} {Rohith Agaram,} {K. Madhava Krishna,} {Srinath Sridhar}},
  journal={NeurIPS},
  year={2023},
  abstract={
  Neural Radiance Fields (NeRF) have become an increasingly popular representation to capture high-quality appearance and shape of scenes and objects. However, learning generalizable NeRF priors over categories of scenes or objects has been challenging due to the high dimensionality of network weight space. To address the limitations of existing work on generalization, multi-view consistency and to improve quality, we propose HyP-NeRF, a latent conditioning method for learning generalizable category-level NeRF priors using hypernetworks. Rather than using hypernetworks to estimate only the weights of a NeRF, we estimate both the weights and the multi-resolution hash encodings resulting in significant quality gains. To improve quality even further, we incorporate a denoise and finetune strategy that denoises images rendered from NeRFs estimated by the hypernetwork and finetunes it while retaining multiview consistency. These improvements enable us to use HyP-NeRF as a generalizable prior for multiple downstream tasks including NeRF reconstruction from single-view or cluttered scenes and text-to-NeRF. We provide qualitative comparisons and evaluate HyP-NeRF on three tasks: generalization, compression, and retrieval, demonstrating our state-of-the-art results.
  },
  pdf={https://arxiv.org/pdf/2306.06093},
  website={https://hyp-nerf.github.io},
  selected={true},
  preview={hypnerf.gif},
}

@inproceedings{sen2023scarp,
  title={SCARP: 3D Shape Completion in ARbitrary Poses for Improved Grasping},
  author={{Bipsaha Sen*,} {Aditya Agarwal*,} {Gaurav Singh*,} {B. Brojeshwar,} {Srinath Sridhar,} {K. Madhava Krishna} },
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={3838--3845},
  year={2023},
  abstract={
  Recovering full 3D shapes from partial observations is a challenging task that has been extensively addressed in the computer vision community. Many deep learning methods tackle this problem by training 3D shape generation networks to learn a prior over the full 3D shapes. In this training regime, the methods expect the inputs to be in a fixed canonical form, without which they fail to learn a valid prior over the 3D shapes. We propose SCARP, a model that performs Shape C ompletion in ARbitrary Poses. Given a partial pointcloud of an object, SCARP learns a disentangled feature representation of pose and shape by relying on rotationally equivariant pose features and geometric shape features trained using a multi-tasking objective. Unlike existing methods that depend on an external canonicalization method, SCARP performs canonicalization, pose estimation, and shape completion in a single network, improving the performance by 45\% over the existing baselines. In this work, we use SCARP for improving grasp proposals on tabletop objects. By completing partial tabletop objects directly in their observed poses, SCARP enables a SOTA grasp proposal network improve their proposals by 71.2\% on partial shapes.
  },
  organization={IEEE},
  pdf={https://arxiv.org/pdf/2301.07213},
  website={https://bipashasen.github.io/scarp},
  selected={true},
  preview={scarp.png},
}